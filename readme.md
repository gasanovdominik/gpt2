# Fine-Tuning GPT-2 для генерации ответов

## Описание проекта
Этот проект выполняет дообучение модели GPT-2 на небольшом наборе данных, содержащем вопросы и ответы в стиле технической поддержки.
Цель — научить модель генерировать осмысленные ответы на вопросы пользователей.

## Шаги выполнения

### 1. Подготовка данных
- Создан небольшой датасет с вопросами и ответами.
- Данные разбиты на обучающую (80%) и тестовую (20%) выборки.

### 2. Загрузка модели и токенизатора
- Используется предобученная модель **GPT-2** из библиотеки `transformers`.
- Токенизатор загружается и настраивается (устанавливается `pad_token`).

### 3. Дообучение модели
- Используется **Trainer API** из библиотеки `transformers`.
- Обучение проводится 3 эпохи с параметрами:
  - `batch_size=2`
  - `weight_decay=0.01`
  - `logging_steps=10`

### 4. Оценка модели
- Модель тестируется на новых запросах.
- Для генерации ответов используется `model.generate()`.

## Запуск проекта

### Установка зависимостей
```bash
pip install transformers datasets torch
```

### Запуск обучения
```bash
python fine_tuning_gpt2.py
```

### Пример тестирования
```python
print(generate_response("Вопрос: Как включить Wi-Fi? Ответ:"))
```

## Результаты
После обучения модель способна генерировать осмысленные ответы на вопросы, подобные тем, что были в обучающем наборе данных. Дополнительно можно улучшить качество, увеличив размер датасета и число эпох обучения.


